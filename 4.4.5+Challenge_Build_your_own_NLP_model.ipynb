{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "hiddenCell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "Collecting ujson>=1.35 (from spacy)\n",
      "Requirement already satisfied: dill<0.3,>=0.2 in /usr/local/envs/py3env/lib/python3.5/site-packages (from spacy)\n",
      "Collecting regex==2017.4.5 (from spacy)\n",
      "Collecting plac<1.0.0,>=0.9.6 (from spacy)\n",
      "  Using cached https://files.pythonhosted.org/packages/9e/9b/62c60d2f5bc135d2aa1d8c8a86aaf84edb719a59c7f11a4316259e61a298/plac-0.9.6-py2.py3-none-any.whl\n",
      "Collecting murmurhash<0.29,>=0.28 (from spacy)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/envs/py3env/lib/python3.5/site-packages (from spacy)\n",
      "Requirement already satisfied: numpy>=1.7 in /usr/local/envs/py3env/lib/python3.5/site-packages (from spacy)\n",
      "Collecting thinc<6.11.0,>=6.10.3 (from spacy)\n",
      "Collecting cymem<1.32,>=1.30 (from spacy)\n",
      "  Using cached https://files.pythonhosted.org/packages/b6/43/39372a0bc24d336dc88b87262c30f09d0a2c759f32a2965f90fb56da46f1/cymem-1.31.2-cp35-cp35m-manylinux1_x86_64.whl\n",
      "Collecting preshed<2.0.0,>=1.0.0 (from spacy)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/envs/py3env/lib/python3.5/site-packages (from requests<3.0.0,>=2.13.0->spacy)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/envs/py3env/lib/python3.5/site-packages (from requests<3.0.0,>=2.13.0->spacy)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/envs/py3env/lib/python3.5/site-packages (from requests<3.0.0,>=2.13.0->spacy)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/envs/py3env/lib/python3.5/site-packages (from requests<3.0.0,>=2.13.0->spacy)\n",
      "Collecting tqdm<5.0.0,>=4.10.0 (from thinc<6.11.0,>=6.10.3->spacy)\n",
      "  Using cached https://files.pythonhosted.org/packages/7d/e6/19dfaff08fcbee7f3453e5b537e65a8364f1945f921a36d08be1e2ff3475/tqdm-4.24.0-py2.py3-none-any.whl\n",
      "Collecting msgpack-numpy<1.0.0,>=0.4.1 (from thinc<6.11.0,>=6.10.3->spacy)\n",
      "  Using cached https://files.pythonhosted.org/packages/84/09/fc890664a7a1dd0a88f46c93fb9340d0a27a69e82095a4a54aef2ed94a6d/msgpack_numpy-0.4.3.1-py2.py3-none-any.whl\n",
      "Collecting wrapt<1.11.0,>=1.10.0 (from thinc<6.11.0,>=6.10.3->spacy)\n",
      "Requirement already satisfied: cytoolz<0.10,>=0.9.0 in /usr/local/envs/py3env/lib/python3.5/site-packages (from thinc<6.11.0,>=6.10.3->spacy)\n",
      "Collecting msgpack<1.0.0,>=0.5.6 (from thinc<6.11.0,>=6.10.3->spacy)\n",
      "  Using cached https://files.pythonhosted.org/packages/08/72/5a01d2a6a894e7f6966b0038445c748d7a16754cceb0e988699269d8152a/msgpack-0.5.6-cp35-cp35m-manylinux1_x86_64.whl\n",
      "Requirement already satisfied: six<2.0.0,>=1.10.0 in /usr/local/envs/py3env/lib/python3.5/site-packages (from thinc<6.11.0,>=6.10.3->spacy)\n",
      "Requirement already satisfied: toolz>=0.8.0 in /usr/local/envs/py3env/lib/python3.5/site-packages (from cytoolz<0.10,>=0.9.0->thinc<6.11.0,>=6.10.3->spacy)\n",
      "Installing collected packages: ujson, regex, plac, murmurhash, tqdm, msgpack, msgpack-numpy, wrapt, cymem, preshed, thinc, spacy\n",
      "Successfully installed cymem-1.31.2 msgpack-0.5.6 msgpack-numpy-0.4.3.1 murmurhash-0.28.0 plac-0.9.6 preshed-1.0.1 regex-2017.4.5 spacy-2.0.12 thinc-6.10.3 tqdm-4.24.0 ujson-1.35 wrapt-1.10.11\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting gcsfs\n",
      "Requirement already satisfied: google-auth>=1.2 in /usr/local/envs/py3env/lib/python3.5/site-packages (from gcsfs)\n",
      "Requirement already satisfied: decorator in /usr/local/envs/py3env/lib/python3.5/site-packages (from gcsfs)\n",
      "Requirement already satisfied: requests in /usr/local/envs/py3env/lib/python3.5/site-packages (from gcsfs)\n",
      "Requirement already satisfied: google-auth-oauthlib in /usr/local/envs/py3env/lib/python3.5/site-packages (from gcsfs)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/envs/py3env/lib/python3.5/site-packages (from google-auth>=1.2->gcsfs)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /usr/local/envs/py3env/lib/python3.5/site-packages (from google-auth>=1.2->gcsfs)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/envs/py3env/lib/python3.5/site-packages (from google-auth>=1.2->gcsfs)\n",
      "Requirement already satisfied: cachetools>=2.0.0 in /usr/local/envs/py3env/lib/python3.5/site-packages (from google-auth>=1.2->gcsfs)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/envs/py3env/lib/python3.5/site-packages (from requests->gcsfs)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/envs/py3env/lib/python3.5/site-packages (from requests->gcsfs)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/envs/py3env/lib/python3.5/site-packages (from requests->gcsfs)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/envs/py3env/lib/python3.5/site-packages (from requests->gcsfs)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/envs/py3env/lib/python3.5/site-packages (from google-auth-oauthlib->gcsfs)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.1 in /usr/local/envs/py3env/lib/python3.5/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs)\n",
      "Requirement already satisfied: oauthlib>=0.6.2 in /usr/local/envs/py3env/lib/python3.5/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs)\n",
      "Installing collected packages: gcsfs\n",
      "Successfully installed gcsfs-0.1.1\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!pip install gcsfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import spacy\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import dask.dataframe as dd\n",
    "from nltk.corpus import stopwords, inaugural\n",
    "import nltk\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect files and pick two to analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1789-Washington.txt',\n",
       " '1793-Washington.txt',\n",
       " '1797-Adams.txt',\n",
       " '1801-Jefferson.txt',\n",
       " '1805-Jefferson.txt',\n",
       " '1809-Madison.txt',\n",
       " '1813-Madison.txt',\n",
       " '1817-Monroe.txt',\n",
       " '1821-Monroe.txt',\n",
       " '1825-Adams.txt',\n",
       " '1829-Jackson.txt',\n",
       " '1833-Jackson.txt',\n",
       " '1837-VanBuren.txt',\n",
       " '1841-Harrison.txt',\n",
       " '1845-Polk.txt',\n",
       " '1849-Taylor.txt',\n",
       " '1853-Pierce.txt',\n",
       " '1857-Buchanan.txt',\n",
       " '1861-Lincoln.txt',\n",
       " '1865-Lincoln.txt',\n",
       " '1869-Grant.txt',\n",
       " '1873-Grant.txt',\n",
       " '1877-Hayes.txt',\n",
       " '1881-Garfield.txt',\n",
       " '1885-Cleveland.txt',\n",
       " '1889-Harrison.txt',\n",
       " '1893-Cleveland.txt',\n",
       " '1897-McKinley.txt',\n",
       " '1901-McKinley.txt',\n",
       " '1905-Roosevelt.txt',\n",
       " '1909-Taft.txt',\n",
       " '1913-Wilson.txt',\n",
       " '1917-Wilson.txt',\n",
       " '1921-Harding.txt',\n",
       " '1925-Coolidge.txt',\n",
       " '1929-Hoover.txt',\n",
       " '1933-Roosevelt.txt',\n",
       " '1937-Roosevelt.txt',\n",
       " '1941-Roosevelt.txt',\n",
       " '1945-Roosevelt.txt',\n",
       " '1949-Truman.txt',\n",
       " '1953-Eisenhower.txt',\n",
       " '1957-Eisenhower.txt',\n",
       " '1961-Kennedy.txt',\n",
       " '1965-Johnson.txt',\n",
       " '1969-Nixon.txt',\n",
       " '1973-Nixon.txt',\n",
       " '1977-Carter.txt',\n",
       " '1981-Reagan.txt',\n",
       " '1985-Reagan.txt',\n",
       " '1989-Bush.txt',\n",
       " '1993-Clinton.txt',\n",
       " '1997-Clinton.txt',\n",
       " '2001-Bush.txt',\n",
       " '2005-Bush.txt',\n",
       " '2009-Obama.txt']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inaugural.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll work with Bush and Obama's inaugural addresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My fellow citizens:\\n\\nI stand here today humbled by the task before us, grateful for the trust you ha'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obama = inaugural.raw('2009-Obama.txt')\n",
    "bush = inaugural.raw('2005-Bush.txt')\n",
    "\n",
    "obama[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up our cleaner function to remove things spacy can't handle and apply it to the obama and bush texts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#text cleaner func\n",
    "def text_cleaner(text):\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\][-]]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Clean data\n",
    "obama_clean = text_cleaner(obama)\n",
    "bush_clean = text_cleaner(bush)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "hiddenCell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz (37.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 37.4MB 16.1MB/s ta 0:00:011    94% |██████████████████████████████▎ | 35.3MB 82.2MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
      "  Running setup.py install for en-core-web-sm ... \u001b[?25l-\b \b\\\b \bdone\n",
      "\u001b[?25hSuccessfully installed en-core-web-sm-2.0.0\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\n",
      "\u001b[93m    Linking successful\u001b[0m\n",
      "    /usr/local/envs/py3env/lib/python3.5/site-packages/en_core_web_sm -->\n",
      "    /usr/local/envs/py3env/lib/python3.5/site-packages/spacy/data/en\n",
      "\n",
      "    You can now load the model via spacy.load('en')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Parse cleaned data\n",
    "nlp = spacy.load('en')\n",
    "obama_doc = nlp(obama_clean)\n",
    "bush_doc = nlp(bush_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Vice, President, Cheney, ,, Mr., Chief, Justi...</td>\n",
       "      <td>bush</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(On, this, day, ,, prescribed, by, law, and, m...</td>\n",
       "      <td>bush</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(I, am, grateful, for, the, honor, of, this, h...</td>\n",
       "      <td>bush</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(At, this, second, gathering, ,, our, duties, ...</td>\n",
       "      <td>bush</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(For, a, half, a, century, ,, America, defende...</td>\n",
       "      <td>bush</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0     1\n",
       "0  (Vice, President, Cheney, ,, Mr., Chief, Justi...  bush\n",
       "1  (On, this, day, ,, prescribed, by, law, and, m...  bush\n",
       "2  (I, am, grateful, for, the, honor, of, this, h...  bush\n",
       "3  (At, this, second, gathering, ,, our, duties, ...  bush\n",
       "4  (For, a, half, a, century, ,, America, defende...  bush"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group into sentences\n",
    "bush_sents = [[sent, 'bush'] for sent in bush_doc.sents]\n",
    "obama_sents = [[sent, 'obma'] for sent in obama_doc.sents]\n",
    "\n",
    "# Combine\n",
    "sentences = pd.DataFrame(bush_sents + obama_sents)\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our data cleaned, parsed, and split into sentences, now we'll create features using bag of words..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create bag of words function for each text\n",
    "def bag_of_words(text):\n",
    "    \n",
    "    # filter out punctuation and stop words\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    # Return most common words\n",
    "    return [item[0] for item in Counter(allwords).most_common(500)]\n",
    "\n",
    "# Get bags \n",
    "bush_words = bag_of_words(bush_doc)\n",
    "obama_words = bag_of_words(obama_doc)\n",
    "\n",
    "# Combine bags to create common set of unique words\n",
    "common_words = set(bush_words + obama_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datum</th>\n",
       "      <th>oppress</th>\n",
       "      <th>mutual</th>\n",
       "      <th>¦</th>\n",
       "      <th>style</th>\n",
       "      <th>cling</th>\n",
       "      <th>soil</th>\n",
       "      <th>leisure</th>\n",
       "      <th>surround</th>\n",
       "      <th>safety</th>\n",
       "      <th>...</th>\n",
       "      <th>planet</th>\n",
       "      <th>worth</th>\n",
       "      <th>effort</th>\n",
       "      <th>slavery</th>\n",
       "      <th>ruler</th>\n",
       "      <th>security</th>\n",
       "      <th>institution</th>\n",
       "      <th>home</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Vice, President, Cheney, ,, Mr., Chief, Justi...</td>\n",
       "      <td>bush</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(On, this, day, ,, prescribed, by, law, and, m...</td>\n",
       "      <td>bush</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(I, am, grateful, for, the, honor, of, this, h...</td>\n",
       "      <td>bush</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(At, this, second, gathering, ,, our, duties, ...</td>\n",
       "      <td>bush</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(For, a, half, a, century, ,, America, defende...</td>\n",
       "      <td>bush</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 832 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  datum oppress mutual  ¦ style cling soil leisure surround safety  \\\n",
       "0     0       0      0  0     0     0    0       0        0      0   \n",
       "1     0       0      0  0     0     0    0       0        0      0   \n",
       "2     0       0      0  0     0     0    0       0        0      0   \n",
       "3     0       0      0  0     0     0    0       0        0      0   \n",
       "4     0       0      0  0     0     0    0       0        0      0   \n",
       "\n",
       "      ...     planet worth effort slavery ruler security institution home  \\\n",
       "0     ...          0     0      0       0     0        0           0    0   \n",
       "1     ...          0     0      0       0     0        0           0    0   \n",
       "2     ...          0     0      0       0     0        0           0    0   \n",
       "3     ...          0     0      0       0     0        0           0    0   \n",
       "4     ...          0     0      0       0     0        0           0    0   \n",
       "\n",
       "                                       text_sentence text_source  \n",
       "0  (Vice, President, Cheney, ,, Mr., Chief, Justi...        bush  \n",
       "1  (On, this, day, ,, prescribed, by, law, and, m...        bush  \n",
       "2  (I, am, grateful, for, the, honor, of, this, h...        bush  \n",
       "3  (At, this, second, gathering, ,, our, duties, ...        bush  \n",
       "4  (For, a, half, a, century, ,, America, defende...        bush  \n",
       "\n",
       "[5 rows x 832 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create bag of words data frame using combined common words and sentences\n",
    "def bow_features(sentences, common_words):\n",
    "    \n",
    "    # Build data frame\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "    df.loc[:, common_words] = 0\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentences in enumerate(df['text_sentence']):\n",
    "        \n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        words = [token.lemma_\n",
    "                 for token in sentences\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "    \n",
    "    return df\n",
    "  \n",
    "  \n",
    "# Create bow features \n",
    "bow = bow_features(sentences, common_words)\n",
    "bow.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "And create features using tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#create sentences\n",
    "obama = inaugural.sents('2009-Obama.txt')\n",
    "bush = inaugural.sents('2005-Bush.txt')\n",
    "\n",
    "# Create list of text \n",
    "obama_list = [\" \".join(sent) for sent in obama]\n",
    "bush_list = [\" \".join(sent) for sent in bush]\n",
    "joined = obama_list + bush_list\n",
    "outcome = ['obama']*len(obama_list) + ['bush']*len(bush_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 94\n",
      "Original sentence: And we will transform our schools and colleges and universities to meet the demands of a new age .\n",
      "Tf_idf vector: {'hope': 0.558188870005978, 'day': 0.5311348441020588, 'chosen': 0.6374331045546121}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(joined, outcome, test_size=0.4, random_state=42)\n",
    "\n",
    "vectorizer = TfidfVectorizer(encoding='ASCII',\n",
    "                             max_df=0.7, # drop words that occur in more than 70% the sentences\n",
    "                             min_df=4, # only use words that appear at least four times\n",
    "                             stop_words='english', \n",
    "                             lowercase=False, #convert everything to lower case\n",
    "                             use_idf=True,#we definitely want to use inverse document frequencies in our weighting\n",
    "                             norm=u'l2', #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "                             smooth_idf=True #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                            )\n",
    "\n",
    "#Applying the vectorizer\n",
    "tfidf=vectorizer.fit_transform(joined)\n",
    "print(\"Number of features: %d\" % tfidf.get_shape()[1])\n",
    "\n",
    "#splitting into training and test sets\n",
    "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tf_idf= train_test_split(tfidf, outcome, test_size=0.4, random_state=0)\n",
    "\n",
    "\n",
    "#Reshapes the vectorizer output into something people can read\n",
    "X_train_tfidf_csr = X_train_tfidf.tocsr()\n",
    "\n",
    "#number of paragraphs\n",
    "n = X_train_tfidf_csr.shape[0]\n",
    "#A list of dictionaries, one per paragraph\n",
    "tfidf_bypara = [{} for _ in range(0,n)]\n",
    "#List of features\n",
    "terms = vectorizer.get_feature_names()\n",
    "#for each paragraph, lists the feature words and their tf-idf scores\n",
    "for i, j in zip(*X_train_tfidf_csr.nonzero()):\n",
    "    tfidf_bypara[i][terms[j]] = X_train_tfidf_csr[i, j]\n",
    "\n",
    "#Keep in mind that the log base 2 of 1 is 0, so a tf-idf score of 0 indicates that the word was present once in that sentence.\n",
    "print('Original sentence:', X_train[5])\n",
    "print('Tf_idf vector:', tfidf_bypara[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent variance captured by all components: 99.65457952151468\n",
      "Component 0:\n",
      "Our goal instead is to help others find their own voice , attain their own freedom , and make their own way .                                                                                                                                                                                                           0.589290\n",
      "This is not primarily the task of arms , though we will defend ourselves and our friends by force of arms when necessary .                                                                                                                                                                                              0.589290\n",
      "Nor is the question before us whether the market is a force for good or ill . Its power to generate wealth and expand freedom is unmatched , but this crisis has reminded us that without a watchful eye , the market can spin out of control -- the nation cannot prosper long when it favors only the prosperous .    0.492541\n",
      "Start on this journey of progress and justice , and America will walk at your side .                                                                                                                                                                                                                                    0.482301\n",
      "Our economy is badly weakened , a consequence of greed and irresponsibility on the part of some , but also our collective failure to make hard choices and prepare the nation for a new age .                                                                                                                           0.464698\n",
      "What the cynics fail to understand is that the ground has shifted beneath them -- that the stale political arguments that have consumed us for so long no longer apply .                                                                                                                                                0.455297\n",
      "So it has been .                                                                                                                                                                                                                                                                                                        0.455297\n",
      "Our nation is at war , against a far - reaching network of violence and hatred .                                                                                                                                                                                                                                        0.447485\n",
      "My most solemn duty is to protect this nation and its people from further attacks and emerging threats .                                                                                                                                                                                                                0.427726\n",
      "Those ideals still light the world , and we will not give them up for expedience ' s sake .                                                                                                                                                                                                                             0.412236\n",
      "Name: 0, dtype: float64\n",
      "Component 1:\n",
      "What the cynics fail to understand is that the ground has shifted beneath them -- that the stale political arguments that have consumed us for so long no longer apply .                                                                                                                                                0.663777\n",
      "So it has been .                                                                                                                                                                                                                                                                                                        0.663777\n",
      "The concerted effort of free nations to promote democracy is a prelude to our enemies ¡¦ defeat .                                                                                                                                                                                                                       0.524056\n",
      "Our nation is at war , against a far - reaching network of violence and hatred .                                                                                                                                                                                                                                        0.510227\n",
      "Nor is the question before us whether the market is a force for good or ill . Its power to generate wealth and expand freedom is unmatched , but this crisis has reminded us that without a watchful eye , the market can spin out of control -- the nation cannot prosper long when it favors only the prosperous .    0.454754\n",
      "At a moment when the outcome of our revolution was most in doubt , the father of our nation ordered these words be read to the people :                                                                                                                                                                                 0.416098\n",
      "God bless you .                                                                                                                                                                                                                                                                                                         0.411695\n",
      "It warms those who feel its power , it burns those who fight its progress , and one day this untamed fire of freedom will reach the darkest corners of our world .                                                                                                                                                      0.389311\n",
      "With hope and virtue , let us brave once more the icy currents , and endure what storms may come .                                                                                                                                                                                                                      0.363235\n",
      "They have been the quiet force of progress throughout our history .                                                                                                                                                                                                                                                     0.362248\n",
      "Name: 1, dtype: float64\n",
      "Component 2:\n",
      "To the Muslim world , we seek a new way forward , based on mutual interest and mutual respect .                                                 0.732926\n",
      "In America ¡¦ s ideal of freedom , the exercise of rights is ennobled by service , and mercy , and a heart for the weak .                       0.585595\n",
      "All this we can do .                                                                                                                            0.509486\n",
      "This is the journey we continue today .                                                                                                         0.508031\n",
      "The best hope for peace in our world is the expansion of freedom in all the world .                                                             0.504252\n",
      "Forty - four Americans have now taken the presidential oath .                                                                                   0.498981\n",
      "After the shipwreck of communism came years of relative quiet , years of repose , years of sabbatical ¡ Xand then there came a day of fire .    0.478854\n",
      "We have confidence because freedom is the permanent hope of mankind , the hunger in dark places , the longing of the soul .                     0.433779\n",
      "It is the honorable achievement of our fathers .                                                                                                0.415382\n",
      "They understood that our power alone cannot protect us , nor does it entitle us to do as we please .                                            0.384957\n",
      "Name: 2, dtype: float64\n",
      "Component 3:\n",
      "After the shipwreck of communism came years of relative quiet , years of repose , years of sabbatical ¡ Xand then there came a day of fire .                                                                                                                                            0.559044\n",
      "This is the journey we continue today .                                                                                                                                                                                                                                                 0.510792\n",
      "The success of our economy has always depended not just on the size of our Gross Domestic Product , but on the reach of our prosperity ; on the ability to extend opportunity to every willing heart -- not out of charity , but because it is the surest route to our common good .    0.492724\n",
      "From the day of our Founding , we have proclaimed that every man and woman on this earth has rights , and dignity , and matchless value , because they bear the image of the Maker of Heaven and earth .                                                                                0.492724\n",
      "Now , there are some who question the scale of our ambitions -- who suggest that our system cannot tolerate too many big plans .                                                                                                                                                        0.370275\n",
      "Our economy is badly weakened , a consequence of greed and irresponsibility on the part of some , but also our collective failure to make hard choices and prepare the nation for a new age .                                                                                           0.358351\n",
      "Make the choice to serve in a cause larger than your wants , larger than yourself ¡ Xand in your days you will add not just to the wealth of our country , but to its character .                                                                                                       0.338339\n",
      "It is the honorable achievement of our fathers .                                                                                                                                                                                                                                        0.324126\n",
      "Bill of Rights .                                                                                                                                                                                                                                                                        0.315454\n",
      "But those values upon which our success depends -- honesty and hard work , courage and fair play , tolerance and curiosity , loyalty and patriotism -- these things are old .                                                                                                           0.314807\n",
      "Name: 3, dtype: float64\n",
      "Component 4:\n",
      "In America ¡¦ s ideal of freedom , citizens find the dignity and security of economic independence , instead of laboring on the edge of subsistence .                                        0.784234\n",
      "My fellow citizens :                                                                                                                                                                         0.657803\n",
      "They are serious and they are many .                                                                                                                                                         0.634186\n",
      "We go forward with complete confidence in the eventual triumph of freedom .                                                                                                                  0.552529\n",
      "The great objective of ending tyranny is the concentrated work of generations .                                                                                                              0.504074\n",
      "Americans move forward in every generation by reaffirming all that is good and true that came before ¡ Xideals of justice and conduct that are the same yesterday , today , and forever .    0.449485\n",
      "On this day , we come to proclaim an end to the petty grievances and false promises , the recriminations and worn - out dogmas that for far too long have strangled our politics .           0.355837\n",
      "Our minds are no less inventive , our goods and services no less needed than they were last week or last month or last year .                                                                0.345838\n",
      "Make the choice to serve in a cause larger than your wants , larger than yourself ¡ Xand in your days you will add not just to the wealth of our country , but to its character .            0.328153\n",
      "But our time of standing pat , of protecting narrow interests and putting off unpleasant decisions -- that time has surely passed .                                                          0.315580\n",
      "Name: 4, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "#Our SVD data reducer.  We are going to reduce the feature space from 94 to 82.\n",
    "svd= TruncatedSVD(82)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "# Run SVD on the training data, then project the training data.\n",
    "X_train_lsa = lsa.fit_transform(X_train_tfidf)\n",
    "\n",
    "variance_explained=svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print(\"Percent variance captured by all components:\",total_variance*100)\n",
    "\n",
    "#Looking at what sorts of paragraphs our solution considers similar, for the first five identified topics\n",
    "paras_by_component=pd.DataFrame(X_train_lsa,index=X_train)\n",
    "for i in range(5):\n",
    "    print('Component {}:'.format(i))\n",
    "    print(paras_by_component.loc[:,i].sort_values(ascending=False)[0:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py3env/lib/python3.5/site-packages/matplotlib/font_manager.py:1320: UserWarning: findfont: Font family ['sans-serif'] not found. Falling back to DejaVu Sans\n",
      "  (prop.get_family(), self.defaultFamily[fontext]))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcgAAAFKCAYAAACZ2c85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAGCBggQAwAIFCQCABQoSAAAL/w//n71Fa3HSUAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1c8816fe48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key:\n",
      "0 We will persistently clarify the choice before every ruler and every nation : The moral choice between oppression , which is always wrong , and freedom , which is eternally right .\n",
      "1 Thank you .\n",
      "2 We go forward with complete confidence in the eventual triumph of freedom .\n",
      "3 I thank President Bush for his service to our nation , as well as the generosity and cooperation he has shown throughout this transition .\n",
      "4 To the people of poor nations , we pledge to work alongside you to make your farms flourish and let clean waters flow ; to nourish starved bodies and feed hungry minds .\n",
      "5 And we will transform our schools and colleges and universities to meet the demands of a new age .\n",
      "6 It is the honorable achievement of our fathers .\n",
      "7 And God bless the United States of America .\n",
      "8 These things are true .\n",
      "9 It is the kindness to take in a stranger when the levees break , the selflessness of workers who would rather cut their hours than see a friend lose their job which sees us through our darkest hours .\n"
     ]
    }
   ],
   "source": [
    "# Compute document similarity using LSA components\n",
    "similarity = np.asarray(np.asmatrix(X_train_lsa) * np.asmatrix(X_train_lsa).T)\n",
    "#Only taking the first 10 sentences\n",
    "sim_matrix=pd.DataFrame(similarity,index=X_train).iloc[0:10,0:10]\n",
    "#Making a plot\n",
    "ax = sns.heatmap(sim_matrix,yticklabels=range(10))\n",
    "plt.show()\n",
    "\n",
    "#Generating a key for the plot.\n",
    "print('Key:')\n",
    "for i in range(10):\n",
    "    print(i,sim_matrix.index[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# BoW \n",
    "X_bow = bow.drop(['text_sentence', 'text_source'], 1)\n",
    "Y_bow = bow['text_source']\n",
    "\n",
    "# Tfidf\n",
    "X_tfidf = X_train_lsa\n",
    "Y_tfidf = y_train_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW Random Forest Scores:  [0.66666667 0.58333333 0.70833333 0.69565217 0.65217391 0.63636364\n",
      " 0.95454545 0.63636364 0.59090909 0.72727273]\n",
      "Avg Score: 0.7022068511198947\n",
      "\n",
      "Tfidf Random Forest Scores: [0.53846154 0.61538462 0.46153846 0.76923077 0.61538462 0.41666667\n",
      " 0.25       0.66666667 0.58333333 0.81818182]\n",
      "Avg Score: 0.6101398601398602\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# BoW\n",
    "rfc = RandomForestClassifier()\n",
    "rfc_bow = rfc.fit(X_bow, Y_bow)\n",
    "print('BoW Random Forest Scores: ', cross_val_score(rfc_bow, X_bow, Y_bow, cv=10))\n",
    "print('Avg Score:', np.mean(cross_val_score(rfc_bow, X_bow, Y_bow, cv=10)))\n",
    "\n",
    "# Tfidf\n",
    "rfc = RandomForestClassifier()\n",
    "rfc_tfidf = rfc.fit(X_tfidf, Y_tfidf)\n",
    "print('\\nTfidf Random Forest Scores:', cross_val_score(rfc_tfidf, X_tfidf, Y_tfidf, cv=10))\n",
    "print('Avg Score:', np.mean(cross_val_score(rfc_tfidf, X_tfidf, Y_tfidf, cv=10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW Logistic Regression Scores:  [0.66666667 0.625      0.70833333 0.86956522 0.73913043 0.72727273\n",
      " 0.86363636 0.72727273 0.59090909 0.81818182]\n",
      "Avg Score: 0.733596837944664\n",
      "\n",
      "Tfidf Logistic Regression Scores: [0.69230769 0.69230769 0.61538462 0.76923077 0.69230769 0.75\n",
      " 0.58333333 0.75       0.75       0.90909091]\n",
      "Avg Score: 0.7203962703962704\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# BoW\n",
    "lr = LogisticRegression()\n",
    "lr_bow = lr.fit(X_bow, Y_bow)\n",
    "print('BoW Logistic Regression Scores: ', cross_val_score(lr_bow, X_bow, Y_bow, cv=10))\n",
    "print('Avg Score:', np.mean(cross_val_score(lr_bow, X_bow, Y_bow, cv=10)))\n",
    "\n",
    "# Tfidf\n",
    "lr = LogisticRegression()\n",
    "lr_tfidf = lr.fit(X_tfidf, Y_tfidf)\n",
    "print('\\nTfidf Logistic Regression Scores:', cross_val_score(lr_tfidf, X_tfidf, Y_tfidf, cv=10))\n",
    "print('Avg Score:', np.mean(cross_val_score(lr_tfidf, X_tfidf, Y_tfidf, cv=10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bow Gradient Boosting Scores: [0.70833333 0.70833333 0.70833333 0.7826087  0.69565217 0.59090909\n",
      " 0.77272727 0.63636364 0.59090909 0.72727273]\n",
      "Avg Score: 0.7044631093544138\n",
      "\n",
      "Tfidf Gradient Boosting Scores: [0.61538462 0.46153846 0.46153846 0.69230769 0.61538462 0.41666667\n",
      " 0.5        0.58333333 0.5        0.90909091]\n",
      "Avg Score: 0.5761655011655011\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# BoW\n",
    "clf = GradientBoostingClassifier()\n",
    "clf_bow = clf.fit(X_bow, Y_bow)\n",
    "print('Bow Gradient Boosting Scores:', cross_val_score(clf_bow, X_bow,Y_bow, cv=10))\n",
    "print('Avg Score:', np.mean(cross_val_score(clf_bow, X_bow, Y_bow, cv=10)))\n",
    "\n",
    "# Tfidf\n",
    "clf = GradientBoostingClassifier()\n",
    "clf_tfidf = clf.fit(X_tfidf, Y_tfidf)\n",
    "print('\\nTfidf Gradient Boosting Scores:', cross_val_score(clf_tfidf, X_tfidf, Y_tfidf, cv=10))\n",
    "print('Avg Score:', np.mean(cross_val_score(clf_tfidf, X_tfidf, Y_tfidf, cv=10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Logistic Regression has the highest score, but its less consitent across folds. I'll use Gradient Boosting as my model and try to improve it by adding Length of Sentence and Parts of speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datum</th>\n",
       "      <th>oppress</th>\n",
       "      <th>mutual</th>\n",
       "      <th>¦</th>\n",
       "      <th>style</th>\n",
       "      <th>cling</th>\n",
       "      <th>soil</th>\n",
       "      <th>leisure</th>\n",
       "      <th>surround</th>\n",
       "      <th>safety</th>\n",
       "      <th>...</th>\n",
       "      <th>security</th>\n",
       "      <th>institution</th>\n",
       "      <th>home</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_source</th>\n",
       "      <th>verbs</th>\n",
       "      <th>adverbs</th>\n",
       "      <th>nouns</th>\n",
       "      <th>puncuation</th>\n",
       "      <th>length_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Vice, President, Cheney, ,, Mr., Chief, Justi...</td>\n",
       "      <td>bush</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(On, this, day, ,, prescribed, by, law, and, m...</td>\n",
       "      <td>bush</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(I, am, grateful, for, the, honor, of, this, h...</td>\n",
       "      <td>bush</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(At, this, second, gathering, ,, our, duties, ...</td>\n",
       "      <td>bush</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(For, a, half, a, century, ,, America, defende...</td>\n",
       "      <td>bush</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 837 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  datum oppress mutual  ¦ style cling soil leisure surround safety  \\\n",
       "0     0       0      0  0     0     0    0       0        0      0   \n",
       "1     0       0      0  0     0     0    0       0        0      0   \n",
       "2     0       0      0  0     0     0    0       0        0      0   \n",
       "3     0       0      0  0     0     0    0       0        0      0   \n",
       "4     0       0      0  0     0     0    0       0        0      0   \n",
       "\n",
       "        ...       security institution home  \\\n",
       "0       ...              0           0    0   \n",
       "1       ...              0           0    0   \n",
       "2       ...              0           0    0   \n",
       "3       ...              0           0    0   \n",
       "4       ...              0           0    0   \n",
       "\n",
       "                                       text_sentence text_source verbs  \\\n",
       "0  (Vice, President, Cheney, ,, Mr., Chief, Justi...        bush    33   \n",
       "1  (On, this, day, ,, prescribed, by, law, and, m...        bush    31   \n",
       "2  (I, am, grateful, for, the, honor, of, this, h...        bush    35   \n",
       "3  (At, this, second, gathering, ,, our, duties, ...        bush    25   \n",
       "4  (For, a, half, a, century, ,, America, defende...        bush    18   \n",
       "\n",
       "  adverbs nouns puncuation length_sentence  \n",
       "0       0     0          5               9  \n",
       "1       0     5          6               4  \n",
       "2       0     8          5               3  \n",
       "3       2     5          4               3  \n",
       "4       0     2          4               2  \n",
       "\n",
       "[5 rows x 837 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parts_of_speech(df):\n",
    "    counts = [] # initialize list\n",
    "    for sentence in df.text_sentence: # loop through rows in df.text_sentence column, intialize vars to zero. \n",
    "        verbs = 0\n",
    "        adverbs = 0\n",
    "        nouns = 0\n",
    "        puncuation = 0\n",
    "        length_sentence = len(sentence)\n",
    "        for token in sentence: #loop through words/tokens in sentence and add one if matched\n",
    "            if token.pos_ == 'ADV':\n",
    "                verbs +=1\n",
    "            elif token.pos_ == 'VERB':\n",
    "                adverbs +=1\n",
    "            elif token.pos_ == 'NOUN':\n",
    "                nouns +=1\n",
    "            elif token.pos_ == 'PUNCT':\n",
    "                puncuation +=1\n",
    "        counts.append([length_sentence, verbs, adverbs, nouns, puncuation]) # append counts to each variable\n",
    "    df_pos = pd.DataFrame.from_records(counts, columns=['verbs', 'adverbs', 'nouns','puncuation', 'length_sentence']) # turn lists into columns\n",
    "    df_concat = pd.concat([df, df_pos], axis =1)\n",
    "    \n",
    "    return df_concat\n",
    "bow_pos = parts_of_speech(bow)\n",
    "\n",
    "bow_pos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bow Gradient Boosting Scores: [0.70833333 0.66666667 0.70833333 0.7826087  0.7826087  0.68181818\n",
      " 0.90909091 0.72727273 0.72727273 0.77272727]\n",
      "Avg Score: 0.7515974967061924\n"
     ]
    }
   ],
   "source": [
    "X_bow_pos = bow_pos.drop(['text_sentence', 'text_source'], 1)\n",
    "\n",
    "# BoW\n",
    "clf = GradientBoostingClassifier()\n",
    "clf_bow = clf.fit(X_bow_pos, Y_bow)\n",
    "print('Bow Gradient Boosting Scores:', cross_val_score(clf_bow, X_bow_pos,Y_bow, cv=10))\n",
    "print('Avg Score:', np.mean(cross_val_score(clf_bow, X_bow_pos, Y_bow, cv=10)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal here was to improve the model by 5%, and we went from 70% to 75% by adding Parts of Speech and length of sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
