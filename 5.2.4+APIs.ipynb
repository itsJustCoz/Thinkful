{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Challenge\n",
    "\n",
    "Do a little scraping or API-calling of your own.  Pick a new website and see what you can get out of it.  Expect that you'll run into bugs and blind alleys, and rely on your mentor to help you get through.  \n",
    "\n",
    "Formally, your goal is to write a scraper that will:\n",
    "\n",
    "1) Return specific pieces of information (rather than just downloading a whole page)  \n",
    "2) Iterate over multiple pages/queries  \n",
    "3) Save the data to your computer  \n",
    "\n",
    "Once you have your data, compute some statistical summaries and/or visualizations that give you some new insights into your scraping topic of interest.  Write up a report from scraping code to summary and share it with your mentor.\n",
    "\n",
    "### Craigslist Hot Tubs For Sale\n",
    "I will scrape all of the hot tubs for sale in the Denver area to analyze sale prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "# Importing in each cell because of the kernel restarts.\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "\n",
    "class CLSpider(scrapy.Spider):\n",
    "    # Naming the spider is important if you are running more than one spider of\n",
    "    # this class simultaneously.\n",
    "    name = \"CL\"\n",
    "    \n",
    "    # URL(s) to start with.\n",
    "    start_urls = [\n",
    "        'https://denver.craigslist.org/search/sss?query=hot%20tub&sort=rel',\n",
    "    ]\n",
    "\n",
    "    # Use XPath to parse the response we get.\n",
    "    def parse(self, response):\n",
    "        \n",
    "        # Iterate over every <article> element on the page.\n",
    "        for posting in response.xpath('//p'):\n",
    "            \n",
    "            # Yield a dictionary with the values we want.\n",
    "            yield {\n",
    "                # This is the code to choose what we want to extract\n",
    "                # You can modify this with other Xpath expressions to extract other information from the site\n",
    "                'title': posting.xpath('a[@class=\"result-title hdrlnk\"]/text()').extract_first(),\n",
    "                'date': posting.xpath('time[@class=\"result-date\"]/text()').extract_first(),\n",
    "                'price': posting.xpath('span/span[@class=\"result-price\"]/text()').extract_first()\n",
    "            }\n",
    "        next_page = response.xpath('//div/div/span[@class=\"buttons\"]/a[3][@href]').extract_first()\n",
    "        \n",
    "        pagenum = int(re.findall(r'\\d+',next_page)[0])\n",
    "        \n",
    "        if next_page is not None and pagenum < 10:\n",
    "            next_page = response.urljoin(next_page)\n",
    "            yield scrapy.Request(next_page, callback=self.parse)\n",
    "\n",
    "# Tell the script how to run the crawler by passing in settings.\n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',         # Store data in JSON format.\n",
    "    'FEED_URI': 'cldata.json',  # Name our storage file.\n",
    "    'ROBOTSTXT_OBEY': True,\n",
    "    'USER_AGENT': 'ThinkfulDataScienceBootcamp_Katherine (thinkful.com)',\n",
    "    'AUTOTHROTTLE_ENABLED': True,\n",
    "    'HTTPCACHE_ENABLED': True,\n",
    "    'LOG_ENABLED': False           # Turn off logging for now.\n",
    "})\n",
    "\n",
    "# Start the crawler with our spider.\n",
    "process.crawl(CLSpider)\n",
    "process.start()\n",
    "print('Success!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the Spider scraped the data, let's create a data frame to display and analyze the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jun  7</td>\n",
       "      <td>$500</td>\n",
       "      <td>Hot tub</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jun  7</td>\n",
       "      <td>$350</td>\n",
       "      <td>Hot tub for Sale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jun  7</td>\n",
       "      <td>None</td>\n",
       "      <td>Hot tub &amp; Spa Cover Super sale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jun  7</td>\n",
       "      <td>None</td>\n",
       "      <td>(A+) JUNK REMOVAL~ HOT TUB REMOVAL~CONCRETE -&amp;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jun  7</td>\n",
       "      <td>$50</td>\n",
       "      <td>Sub panel hot tub ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     date price                                              title\n",
       "0  Jun  7  $500                                            Hot tub\n",
       "1  Jun  7  $350                                   Hot tub for Sale\n",
       "2  Jun  7  None                     Hot tub & Spa Cover Super sale\n",
       "3  Jun  7  None  (A+) JUNK REMOVAL~ HOT TUB REMOVAL~CONCRETE -&...\n",
       "4  Jun  7   $50                                Sub panel hot tub ?"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Turning JSON into Data Frame\n",
    "tubs = pd.read_json('cldata.json')\n",
    "print(tubs.shape)\n",
    "tubs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! This worked, but I can't seem to get the next pages to work.  I'll write these up for now.\n",
    "\n",
    "Next step will be to clean out the dollar signs from the price and then turning the non-\"None\" values into integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jun  7</td>\n",
       "      <td>500.0</td>\n",
       "      <td>Hot tub</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jun  7</td>\n",
       "      <td>350.0</td>\n",
       "      <td>Hot tub for Sale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jun  7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hot tub &amp; Spa Cover Super sale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jun  7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(A+) JUNK REMOVAL~ HOT TUB REMOVAL~CONCRETE -&amp;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jun  7</td>\n",
       "      <td>50.0</td>\n",
       "      <td>Sub panel hot tub ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     date  price                                              title\n",
       "0  Jun  7  500.0                                            Hot tub\n",
       "1  Jun  7  350.0                                   Hot tub for Sale\n",
       "2  Jun  7    NaN                     Hot tub & Spa Cover Super sale\n",
       "3  Jun  7    NaN  (A+) JUNK REMOVAL~ HOT TUB REMOVAL~CONCRETE -&...\n",
       "4  Jun  7   50.0                                Sub panel hot tub ?"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "tubs.price = tubs.price.map(lambda x: None if x == None else int(re.sub('\\$', '', str(x))))\n",
    "tubs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fantastic! Now, let's find the average price for a hot tub in the denver area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1088.1904761904761\n"
     ]
    }
   ],
   "source": [
    "tubwprice = tubs[tubs.price>0]\n",
    "average_price = tubwprice.price.mean()\n",
    "print(average_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the average price for a hot tub is \\$1088. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114b39ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.hist(tubwprice.price, bins=20)\n",
    "plt.xlabel('Tub Price')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Price of Hot Tubs on Denver Craigslist')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the average price listed is \\$1088, it appears that most hot tubs are listed for under \\$500, with around 17 listed around \\$1500 and some listed at a significantly higher price. \n",
    "\n",
    "## Failed Attempt\n",
    "I really wanted to get the below Indeed Spider to work, but couldn't.  Thoughts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-07 18:59:53 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: scrapybot)\n",
      "2018-06-07 18:59:53 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.6.2 (v3.6.2:5fd33b5926, Jul 16 2017, 20:11:06) - [GCC 4.2.1 (Apple Inc. build 5666) (dot 3)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Darwin-17.5.0-x86_64-i386-64bit\n",
      "2018-06-07 18:59:53 [scrapy.crawler] INFO: Overridden settings: {'AUTOTHROTTLE_ENABLED': True, 'FEED_FORMAT': 'json', 'FEED_URI': 'firstpageindeed.json', 'HTTPCACHE_ENABLED': True, 'ROBOTSTXT_OBEY': True, 'USER_AGENT': 'ThinkfulDataScienceBootcamp_Katherine (thinkful.com)'}\n",
      "2018-06-07 18:59:53 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats',\n",
      " 'scrapy.extensions.throttle.AutoThrottle']\n",
      "2018-06-07 18:59:53 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats',\n",
      " 'scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware']\n",
      "2018-06-07 18:59:53 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2018-06-07 18:59:53 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2018-06-07 18:59:53 [scrapy.core.engine] INFO: Spider opened\n",
      "2018-06-07 18:59:53 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2018-06-07 18:59:53 [scrapy.extensions.httpcache] DEBUG: Using filesystem cache storage in .scrapy/httpcache\n",
      "2018-06-07 18:59:53 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023\n",
      "2018-06-07 18:59:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.indeed.com/robots.txt> (referer: None) ['cached']\n",
      "2018-06-07 18:59:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.indeed.com/jobs?q=data+scientist&l=Denver,+CO&explvl=entry_level> (referer: None) ['cached']\n",
      "2018-06-07 18:59:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.indeed.com/jobs?q=data+scientist&l=Denver,+CO&explvl=entry_level> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/parsel/selector.py\", line 228, in xpath\n",
      "    **kwargs)\n",
      "  File \"src/lxml/etree.pyx\", line 1577, in lxml.etree._Element.xpath\n",
      "  File \"src/lxml/xpath.pxi\", line 307, in lxml.etree.XPathElementEvaluator.__call__\n",
      "  File \"src/lxml/xpath.pxi\", line 227, in lxml.etree._XPathEvaluatorBase._handle_result\n",
      "lxml.etree.XPathEvalError: Invalid expression\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scrapy/utils/defer.py\", line 102, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py\", line 30, in process_spider_output\n",
      "    for x in result:\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py\", line 339, in <genexpr>\n",
      "    return (_set_referer(r) for r in result or ())\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py\", line 37, in <genexpr>\n",
      "    return (r for r in result or () if _filter(r))\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py\", line 58, in <genexpr>\n",
      "    return (r for r in result or () if _filter(r))\n",
      "  File \"<ipython-input-1-7f058ce442e6>\", line 20, in parse\n",
      "    for posting in response.xpath('//*div[contains(@class,\"result\")]'):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scrapy/http/response/text.py\", line 119, in xpath\n",
      "    return self.selector.xpath(query, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/parsel/selector.py\", line 232, in xpath\n",
      "    six.reraise(ValueError, ValueError(msg), sys.exc_info()[2])\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/six.py\", line 692, in reraise\n",
      "    raise value.with_traceback(tb)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/parsel/selector.py\", line 228, in xpath\n",
      "    **kwargs)\n",
      "  File \"src/lxml/etree.pyx\", line 1577, in lxml.etree._Element.xpath\n",
      "  File \"src/lxml/xpath.pxi\", line 307, in lxml.etree.XPathElementEvaluator.__call__\n",
      "  File \"src/lxml/xpath.pxi\", line 227, in lxml.etree._XPathEvaluatorBase._handle_result\n",
      "ValueError: XPath error: Invalid expression in //*div[contains(@class,\"result\")]\n",
      "2018-06-07 18:59:53 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2018-06-07 18:59:53 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 631,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 60380,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2018, 6, 8, 0, 59, 53, 907228),\n",
      " 'httpcache/hit': 2,\n",
      " 'log_count/DEBUG': 4,\n",
      " 'log_count/ERROR': 1,\n",
      " 'log_count/INFO': 7,\n",
      " 'memusage/max': 88289280,\n",
      " 'memusage/startup': 88289280,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'spider_exceptions/ValueError': 1,\n",
      " 'start_time': datetime.datetime(2018, 6, 8, 0, 59, 53, 740351)}\n",
      "2018-06-07 18:59:53 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "# Importing in each cell because of the kernel restarts.\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "\n",
    "class IndSpider(scrapy.Spider):\n",
    "    # Naming the spider is important if you are running more than one spider of\n",
    "    # this class simultaneously.\n",
    "    name = \"Ind\"\n",
    "    \n",
    "    # URL(s) to start with.\n",
    "    start_urls = [\n",
    "        'https://www.indeed.com/jobs?q=data+scientist&l=Denver,+CO&explvl=entry_level',\n",
    "    ]\n",
    "\n",
    "    # Use XPath to parse the response we get.\n",
    "    def parse(self, response):\n",
    "        \n",
    "        # Iterate over every <article> element on the page.\n",
    "        for posting in response.xpath('//*div[contains(@class,\"result\")]'):\n",
    "            \n",
    "            # Yield a dictionary with the values we want.\n",
    "            yield {\n",
    "                # This is the code to choose what we want to extract\n",
    "                # You can modify this with other Xpath expressions to extract other information from the site\n",
    "                'job_title': posting.xpath('h2/a[@title]/text()').extract(),\n",
    "                'company': posting.xpath('span[@class=\"company\"]/a/text()').extract(),\n",
    "                'summary': posting.xpath('table/tbody/tr/td/div/span/text()').extract()\n",
    "            }\n",
    "\n",
    "# Tell the script how to run the crawler by passing in settings.\n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',         # Store data in JSON format.\n",
    "    'FEED_URI': 'firstpageindeed.json',  # Name our storage file.\n",
    "    'ROBOTSTXT_OBEY': True,\n",
    "    'USER_AGENT': 'ThinkfulDataScienceBootcamp_Katherine (thinkful.com)',\n",
    "    'AUTOTHROTTLE_ENABLED': True,\n",
    "    'HTTPCACHE_ENABLED': True,\n",
    "    'LOG_ENABLED': True           # Turn off logging for now.\n",
    "})\n",
    "\n",
    "# Start the crawler with our spider.\n",
    "process.crawl(IndSpider)\n",
    "process.start()\n",
    "print('Success!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This scraper keeps either exporting an empty file or it won't "
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
